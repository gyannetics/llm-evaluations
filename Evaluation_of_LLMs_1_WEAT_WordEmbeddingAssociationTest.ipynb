{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+mwW6t+7fOqPyMn8grW4h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyannetics/llm-evaluations/blob/main/Evaluation_of_LLMs_1_WEAT_WordEmbeddingAssociationTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of LLMs : 1. WEAT - WordEmbeddingAssociationTest"
      ],
      "metadata": {
        "id": "ngyVI0XEfWTG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ypaANm-Yd-Rk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We are trying to find the association between Professions and Genders and identify if there is any bias"
      ],
      "metadata": {
        "id": "zoDi5En9rkaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['doctor', 'engineer', 'scientist'] # TargetSet 1\n",
        "Y = ['nurse', 'teacher', 'receptionist'] # TargetSet 2\n",
        "\n",
        "# Gender Specific Attribute Sets\n",
        "A = ['man', 'male'] # AttributeSet 1\n",
        "B = ['woman', 'female'] # Attribute Set2"
      ],
      "metadata": {
        "id": "PwzSaf0GkmTR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We shall create some embeddings:\n",
        "- with BERT\n",
        "- Random"
      ],
      "metadata": {
        "id": "c9NQBxUfhRa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to generate embeddings\n",
        "def get_embeddings(word_list):\n",
        "    embeddings_dict = {}\n",
        "    with torch.no_grad():  # No need to calculate gradients\n",
        "        for word in word_list:\n",
        "            inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "            outputs = model(**inputs)\n",
        "            # Take the embeddings from the last hidden state\n",
        "            # The shape is [batch_size, sequence_length, hidden_size]\n",
        "            # We take the mean of the sequence_length dimension to get a single vector\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "            embeddings_dict[word] = embeddings\n",
        "    return embeddings_dict\n",
        "\n",
        "# Combine lists\n",
        "words = ['doctor', 'engineer', 'scientist', 'nurse', 'teacher', 'receptionist', 'man', 'male',  'woman', 'female']\n",
        "\n",
        "# Get embeddings\n",
        "bert_embeddings = get_embeddings(words)\n",
        "\n",
        "# Print the shape of the embedding for the first word to verify\n",
        "print(f\"Shape of '{words[0]}' embedding:\", bert_embeddings[words[0]].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVY73f5qmkL0",
        "outputId": "06f25cac-e4ec-4a5f-d75d-8a9ebb46df7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of 'doctor' embedding: torch.Size([1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly created embeddings to the above items in the lists and return them as a dictionary for each item in the list\n",
        "\n",
        "def create_word_embeddings(X, Y, A, B):\n",
        "  # Concatenate all the lists into a single list\n",
        "  all_words = X + Y + A + B\n",
        "\n",
        "  # Create a dictionary to store the word embeddings\n",
        "  word_embeddings = {}\n",
        "\n",
        "  # Iterate over the list of words and create a random embedding for each word\n",
        "  for word in all_words:\n",
        "    word_embeddings[word] = np.random.rand(100)  # Replace 100 with the desired embedding dimension\n",
        "\n",
        "  return word_embeddings\n",
        "\n",
        "# Call the function to create word embeddings\n",
        "random_word_embeddings = create_word_embeddings(X, Y, A, B)\n",
        "\n",
        "# Print the word embeddings\n",
        "print(random_word_embeddings['doctor'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwAI4lQJlZYc",
        "outputId": "735585b1-2101-44a7-8ee6-9d1150b1b4ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.14232806 0.42139195 0.19024858 0.20635999 0.80903024 0.11449117\n",
            " 0.00269196 0.38712229 0.59822856 0.48121422 0.59751105 0.99721589\n",
            " 0.09963012 0.26438179 0.32251013 0.00608766 0.84281799 0.16766294\n",
            " 0.77109816 0.22248067 0.5904264  0.74991374 0.61879626 0.01309462\n",
            " 0.74248081 0.61864603 0.55338115 0.68814519 0.671497   0.34228767\n",
            " 0.03267136 0.12143251 0.7304639  0.88682273 0.08920331 0.84305402\n",
            " 0.39723113 0.56468366 0.14052809 0.99747966 0.07766593 0.32079182\n",
            " 0.36080029 0.23112819 0.34233689 0.05958251 0.55565019 0.79410585\n",
            " 0.39852353 0.52939022 0.39279736 0.50647827 0.99629866 0.43425582\n",
            " 0.8676812  0.89327358 0.96536766 0.63738118 0.11693651 0.48287501\n",
            " 0.30077922 0.47094902 0.57248002 0.00939154 0.50947761 0.85094862\n",
            " 0.98879147 0.69013048 0.05611937 0.86059763 0.32675977 0.73822196\n",
            " 0.79282388 0.85546324 0.06293241 0.43475501 0.80354317 0.12491492\n",
            " 0.80883719 0.8305406  0.85395285 0.84245165 0.38968252 0.96305057\n",
            " 0.05587253 0.91572294 0.29036239 0.6756238  0.02371534 0.87694069\n",
            " 0.34224292 0.56731626 0.06790087 0.67109829 0.56796733 0.25931731\n",
            " 0.35826534 0.70207816 0.51203376 0.07488535]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's examine with BERT embeddings"
      ],
      "metadata": {
        "id": "rF2EC9hTnCuZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWCZSM_UnU-9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def similarity(w, X, Y, word_embeddings):\n",
        "    w_embedding = word_embeddings[w].reshape(1, -1)  # Ensure w_embedding is 2D\n",
        "    X_embeddings = np.vstack([word_embeddings[x] for x in X])  # Stack embeddings vertically\n",
        "    Y_embeddings = np.vstack([word_embeddings[y] for y in Y])  # Stack embeddings vertically\n",
        "\n",
        "    sim_X = np.mean(cosine_similarity(w_embedding, X_embeddings))\n",
        "    sim_Y = np.mean(cosine_similarity(w_embedding, Y_embeddings))\n",
        "\n",
        "    return sim_X - sim_Y\n",
        "\n",
        "def WEAT_Score(A, B, X, Y, word_embeddings):\n",
        "    score_A = np.sum([similarity(a, X, Y, word_embeddings) for a in A])\n",
        "    score_B = np.sum([similarity(b, X, Y, word_embeddings) for b in B])\n",
        "\n",
        "    return score_A - score_B\n"
      ],
      "metadata": {
        "id": "jtKV8KQOnJKd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_weat_score = WEAT_Score(A, B, X, Y, bert_embeddings)\n",
        "random_weat_score = WEAT_Score(A, B, X, Y, random_word_embeddings)"
      ],
      "metadata": {
        "id": "_HOAYQVcoem7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_weat_score, random_weat_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1l6_hlJpDWQ",
        "outputId": "d616f297-1c26-40c8-96ff-6daf72572c73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.017807066 -0.004144265256856183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For practical applications, BERT-based embeddings are preferable due to their rich semantic representation, despite the slight bias indicated by the WEAT score."
      ],
      "metadata": {
        "id": "77ahbOmJFiZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEAT Score Interpretation\n",
        "- **Positive Score:** Indicates a stronger association of the first target set (X) with the first attribute set (A) and the second target set (Y) with the second attribute set (B), or vice versa, depending on the specific sets and their semantic meanings.\n",
        "- **Negative Score:** Indicates a stronger association of the first target set with the second attribute set and the second target set with the first attribute set.\n",
        "- **Score Closer to Zero:** Suggests a weaker differential association between the target sets and the attribute sets, implying less bias in the context of the WEAT test.\n",
        "# Comparing the Scores\n",
        "- **BERT-based Embeddings (0.017807066):** This positive score suggests a slight association bias according to the definitions of your target and attribute sets. However, the score is relatively close to zero, indicating that the bias, while present, is not pronounced. Given BERT's training on a diverse and extensive corpus, it's promising to see a low magnitude of bias, though the ideal would be even closer to zero.\n",
        "\n",
        "- **Random Numbers Based Embeddings (-0.004144265256856183):** This negative score is very close to zero, suggesting a minimal differential association and, thus, minimal bias within the context of our WEAT test. Interestingly, despite being generated randomly, these embeddings yield a result suggesting minimal bias in terms of differential association measured by WEAT. However, random embeddings lack meaningful semantic relationships, so their near-zero score doesn't imply they're suitable for tasks requiring semantic understanding."
      ],
      "metadata": {
        "id": "-WaevXSpGorg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCwJiWRRpU4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}